# What Does BERT Look at? An Analysis of BERT’s Attention

本解析では、BERT の内部 Attention マップを様々な観点から解析し、内部構造や言語情報の獲得メカニズムを明らかにしています。コードは主に以下の処理を行っています。

- **BERTモデル内部の Q, K, V の抽出（Section 0）**
- **Attention マップの抽出とキャッシュ、ならびに Jensen–Shannon Divergence の計算（Section 1）**
- **内部表現の次元削減と抽出（Section 2）**
- **Plotly や igraph を用いた可視化ユーティリティの実装（Section 3～4）**
- **Attention ヘッドのクラスタリング（Section 5）**
- **全体の可視化ページのレンダリング（Section 6）**

以下、各セクションごとに詳細を示します。

---

## 0. カスタム BERT クラス（内部から Q, K, V を抽出するためのフック付き）

### 実装内容
- クラス `BertModelWithQKV` は、BERT モデルを継承し、各レイヤーでのクエリ \( Q_i \)、キー \( K_i \)、バリュー \( V_i \) を forward hook を利用して抽出します。  
- 各レイヤーの attention モジュール内で、入力の隠れ状態 \( h_i \) から下記の数式に基づいて計算される内部表現を得、キャッシュに保存します。

### 数式
$$
Q_i = W_Q \cdot h_i,\quad K_i = W_K \cdot h_i,\quad V_i = W_V \cdot h_i
$$

$$
\alpha_{ij} = \frac{\exp\!\Bigl(\frac{Q_i \cdot K_j}{\sqrt{d_k}}\Bigr)}{\sum_{l=1}^{n}\exp\!\Bigl(\frac{Q_i \cdot K_l}{\sqrt{d_k}}\Bigr)}
$$

### 意義

- 各レイヤー内で、入力トークン \(i\) の表現からクエリ \(Q_i\) が生成され、同じレイヤー内の全トークン \(j\) からキー \(K_j\) が生成されます。  
- Attention 重み \( \alpha_{ij} \) は、トークン \(i\) がどのトークン \(j\) に注目するかの「度合い」を示す確率分布となり、各 \(i\) について \(\sum_{j}\alpha_{ij}=1\) になります。
- 各レイヤーは独立に計算され、前層の出力（または初期埋め込み）が次層の入力になるため、\(i\) と \(j\) はそのレイヤーにおける入力系列中のトークン位置を意味し、各レイヤーごとに異なる文脈情報が反映されます。

- この部分では、各レイヤーでどのトークンがどの程度他のトークンと関連しているか（Attention 重み）が計算される仕組みを抽出するため、後の Attention マップの解析や可視化に不可欠な情報が得られます。
  
Vの更新は、Attentionメカニズムにおいて次のように行われます。各トークン \( i \) の出力表現 \( o_i \) は、全トークン \( j \) からの値ベクトル \( v_j \) を、Attention重み \( \alpha_{ij} \) を使って重み付き和として集約することで求められます。すなわち、

\[
o_i = \sum_{j=1}^{n} \alpha_{ij} \, v_j
\]

この \( o_i \) が、次の層への入力として利用されます。さらに、Transformerでは通常、この出力に線形変換や残差接続、Layer Normalizationが適用されます。

要するに、V自体が直接「更新」されるわけではなく、各トークンの出力がAttention重みを使ってVの値の線形結合として計算され、その結果が次層の入力となるのです。
---

## 1. Attention マップの抽出とキャッシュ

### 実装内容
- 関数 `get_attention_data` は、入力テキストをトークナイズし、BERT モデルにより Attention マップを抽出します。  
- Attention マップは  
  $$ (\text{num\_layers}, \text{num\_heads}, \text{seq\_len}, \text{seq\_len}) $$
  の NumPy 配列として保存され、キャッシュに保存されることで、再実行時に高速に読み込めます。

### 数式
先述の注意重みの計算式：
$$
\alpha_{ij} = \frac{\exp\!\Bigl(\frac{Q_i \cdot K_j}{\sqrt{d_k}}\Bigr)}{\sum_{l=1}^{n}\exp\!\Bigl(\frac{Q_i \cdot K_l}{\sqrt{d_k}}\Bigr)}
$$

### Jensen–Shannon Divergence の計算
- 関数 `jensen_shannon_divergence` および `compute_js_divergences` により、Attention ヘッド間の注意分布の類似性を評価します。  
- 各ヘッド \( H_i \) と \( H_j \) について、全トークン \( t \) ごとに以下の JS Divergence を計算し、その平均値を距離 \( D_{JS}(H_i, H_j) \) として定義します。

#### 数式
$$
JS(P \parallel Q) = \frac{1}{2} \, D_{KL}\!\left(P \parallel \frac{P+Q}{2}\right) + \frac{1}{2} \, D_{KL}\!\left(Q \parallel \frac{P+Q}{2}\right)
$$

$$
D_{JS}(H_i, H_j) = \sum_{\text{token} \in \text{data}} JS\!\Bigl(H_i(\text{token}) \parallel H_j(\text{token})\Bigr)
$$

### 意義
- Attention ヘッド間の類似性を定量的に評価することで、どのヘッドが似た役割を担っているか、また異なるパターンを示しているかを把握し、後のクラスタリングに利用します。

---

## 2. 次元削減と内部表現の抽出

### 実装内容
- 関数 `fit_umap_or_pca_per_head` では、各ヘッドのバリュー \( V \) の内部表現を抽出し、UMAP または PCA を用いて 2 次元に次元削減します。  
- 各ヘッドごとに学習された reducer を用いて、後のグラフ可視化に利用する座標を得ます。

#### 数式
$$
V_i = W_V \cdot h_i
$$

次元削減後の表現は、
$$
\text{Reduced Coordinates} = \text{Reducer}(V)
$$
として得られます。

### 意義
- 内部表現の 2 次元埋め込みにより、各 Attention ヘッドがどのような特徴を持っているかを視覚的に比較・解析できます。

---

## 3. 平均 Attention の計算と Attention エントロピーの計算

### 3.1 平均 Attention の計算

#### 実装内容
- コード内では、各トークンペア \( (i, j) \) の注意重み \( \alpha_{ij} \) に対し、特定のトークンタイプ（[SEP]、[CLS]、句読点など）に対応するマスク \( M_{ij}^{[X]} \) を乗じ、その平均値を計算します。  
- たとえば、[SEP] トークンに対する平均 Attention は、以下のように計算されます。

#### 数式
$$
\text{AvgAttn}^{[SEP]} = \frac{1}{N_{[SEP]}} \sum_{i,j} \alpha_{ij}\, M_{ij}^{[SEP]}
$$

ここで、
- \( M_{ij}^{[SEP]} = \begin{cases} 1, & \text{if token } j \text{ is [SEP]} \\ 0, & \text{otherwise} \end{cases} \)
- \( N_{[SEP]} \) は、マスク \( M_{ij}^{[SEP]} \) の総和または正規化定数です。

### 3.2 Attention エントロピーの計算

#### 実装内容
- 各ヘッドの注意分布のエントロピー \( H(\alpha_i) \) を計算します。  
- これにより、Attention の集中度（低エントロピー）と分散度（高エントロピー）が定量的に評価されます。

#### 数式
$$
H(\alpha_i) = -\sum_{j=1}^{n} \alpha_{ij}\log\alpha_{ij}
$$

### 意義
- 平均 Attention は、各ヘッドが特定のトークンにどの程度注意を向けているかを示し、エントロピーは注意がどれだけ広く分散しているかを示します。  
- これらの計算結果は、Figure 2（平均 Attention プロット）や Figure 4（Attention エントロピー プロット）として可視化され、Attention の性質の理解に役立ちます。

---

## 4. 可視化用プロットユーティリティ

### 4.1 igraph を用いた Attention マップの可視化（Figure 1）

#### 実装内容
- 関数 `plot_attn_igraph_svg` は、指定されたレイヤー・ヘッドの Attention マトリックスを用いて、igraph で SVG 形式のグラフを生成します。  
- グラフは、左側に入力トークン、右側に同じトークンを配置し、トークン間の Attention 重み \( \alpha_{ij} \) をエッジとして描画します。エッジの太さと色は、Attention の強さに比例して変化します。

#### 数式
$$
\alpha_{ij} = \frac{\exp\!\Bigl(\frac{Q_i \cdot K_j}{\sqrt{d_k}}\Bigr)}{\sum_{l=1}^{n}\exp\!\Bigl(\frac{Q_i \cdot K_l}{\sqrt{d_k}}\Bigr)}
$$

#### 意義
- Figure 1 により、各トークン間でどのように Attention が分配されているかを直感的に把握でき、Attention マップの局所的・グローバルな特性を理解する助けとなります。

### 4.2 Plotly を用いた平均 Attention プロット（Figure 2）

#### 実装内容
- 関数 `plot_avg_attention_plotly` は、各グループ（[CLS]、[SEP]、句読点、左右、自己など）ごとの平均 Attention 値を、レイヤーごとに Plotly のサブプロットとして表示します。
  
#### 数式の引用例
$$
\text{AvgAttn}^{[X]} = \frac{1}{N_{[X]}} \sum_{i,j} \alpha_{ij}\, M_{ij}^{[X]}
$$

ここで：
- \( [X] \) には [CLS]、[SEP]、句読点などが該当します。
- \(\alpha_{ij}\) は、入力系列中のトークン \(i\) からトークン \(j\) への注意重みです。
- \(M_{ij}^{[SEP]}\) は、マスク行列として定義され、**トークン \(j\) が [SEP] であれば 1、そうでなければ 0** となります。  
  コード中では、各トークン位置について、もしトークンが "[SEP]" であれば、その位置に対して 1 をセットするベクトル `seps` を作成し、その後、`np.tile(seps, (n_tokens, 1))` によって、各行に同じ `seps` ベクトルを繰り返し、全体のマスク行列 \(M^{[SEP]}\) を作成しています。
- \(N_{[SEP]}\) は、[SEP] に対応するマスクの合計、すなわち全体で [SEP] トークンが現れる回数（または適切な正規化定数）です。  
  コードでは、マスクの合計や、場合によっては行数 \(n_{\text{tokens}}\) で割ることで正規化しています。

この計算により、各レイヤー・ヘッドごとに [SEP] トークンに対してどの程度注意が向けられているかの平均値が得られ、後の可視化（Figure 2 など）やクラスタリングの評価に利用されます。

#### 意義
- このプロット（Figure 2）は、各レイヤーにおける特定トークンタイプへの平均 Attention の変化を示し、どの層でどのトークンに強い注意が向けられているかを視覚的に確認できます。

### 4.3 Plotly を用いた Attention エントロピー プロット（Figure 4）

#### 実装内容
- 関数 `plot_entropy_plotly` は、各レイヤー・ヘッドの注意エントロピー \( H(\alpha_i) \) と、特に [CLS] トークンに対するエントロピーを Plotly でプロットします。

#### 数式
$$
H(\alpha_i) = -\sum_{j=1}^{n} \alpha_{ij}\log\alpha_{ij}
$$

#### 意義
- Figure 4 では、Attention エントロピーの分布がレイヤーごとに示され、Attention がどれだけ集中しているか（低エントロピー）または広く分散しているか（高エントロピー）が明らかになります。


---

## 5. Attention ヘッドのクラスタリング（Figure 6）

### 5.1 ヘッド間距離の定義：JS Divergence

#### 実装内容
- 関数 `compute_js_divergences` は、全 Attention ヘッドの注意分布をフラット化し、各トークンごとに Jensen–Shannon Divergence を計算します。  
- ヘッド間の距離は、各トークンに対する JS Divergence の平均値として定義されます。

#### 数式
$$
D_{JS}(H_i, H_j) = \sum_{\text{token} \in \text{data}} JS\!\Bigl(H_i(\text{token}) \parallel H_j(\text{token})\Bigr)
$$
$$
JS(P \parallel Q) = \frac{1}{2}KL\!\Bigl(P \parallel \frac{P+Q}{2}\Bigr) + \frac{1}{2}KL\!\Bigl(Q \parallel \frac{P+Q}{2}\Bigr)
$$

#### 意義
- この計算により、Attention ヘッド間の機能的な類似性が定量的に評価され、同一層内の冗長性や各ヘッドの役割の多様性が明らかになります。

### 5.2 多次元尺度構成法 (MDS) による埋め込み

#### 実装内容
- 関数 `cached_mds` を用いて、JS Divergence 行列から MDS によって各ヘッドを 2 次元空間に埋め込みます。  
- この埋め込み結果は、各ヘッドの 2 次元座標 \( \mathbf{p}_i \) として得られ、以下の関係が近似されます：
  
  $$
  \|\mathbf{p}_i - \mathbf{p}_j\|_2 \approx D_{JS}(H_i, H_j)
  $$

#### 意義
- Figure 6 のクラスタリングプロットでは、同一層内のヘッドが互いに近い位置に配置されるか、または異なる層・異なる機能のヘッドが分離しているかを視覚的に確認でき、Attention の内部構造や冗長性を理解するための有用な指針となります。

### 5.3 Plotly を用いたクラスタリング結果の可視化

#### 実装内容
- 関数 `plot_cluster_heads` は、MDS により得られた 2 次元埋め込み結果を Plotly でプロットし、各ヘッドの平均 Attention 値やエントロピー、特殊トークンへの注意などの指標に基づいて、色分けやマーカー分けを行います。  
- たとえば、AvgAttn\(_{\text{right}} > 0.5\)、AvgAttn\(_{\text{left}} > 0.5\)、\( H(\alpha) > 3.8 \)、AvgAttn\(_{\text{cls}} > 0.6\)、AvgAttn\(_{\text{sep}} > 0.6\)、AvgAttn\(_{\text{punct}} > 0.6 \) などの閾値を用いて、Attention ヘッドの行動を分類しています。

#### 意義
- Figure 6 では、各ヘッドの 2 次元埋め込みにより、似た注意パターンを持つヘッドがどの程度クラスタを形成しているかが確認でき、Attention の分散表現の冗長性や多様性を視覚的に理解できます。

<!-- ---

## 6. 全体の可視化ページのレンダリング

### 実装内容
- 関数 `render_page` は、Streamlit を用いて全体の解析結果をインタラクティブに表示するページを構築します。
- ユーザーはテキスト入力欄から解析対象のテキストを指定し、以下の各グラフや結果を確認できます。

### 表示されるグラフ
- **Figure 1 (Attention マップの例):**  
  - igraph を用いて、選択したレイヤー・ヘッドの Attention マトリックスを SVG グラフとして表示します。  
  - このグラフは、各トークン間の Attention 重み \( \alpha_{ij} \) を線の太さと色（青の透明度）で表現し、どのトークンがどのトークンに強い注意を向けているかを示します。

- **Figure 2 (平均 Attention プロット):**  
  - Plotly を用いて、各 Attention ヘッドが [CLS]、[SEP]、句読点、左右（next, prev）、自己（current token）など、特定のトークンタイプに対してどれだけ注意を向けているかの平均値をレイヤーごとにプロットします。  
  - 数式  
    $$
    \text{AvgAttn}^{[X]} = \frac{1}{N_{[X]}} \sum_{i,j} \alpha_{ij}\, M_{ij}^{[X]}
    $$
    に基づいて計算された値が、グループ別に散布図と平均線として表示されます。

- **Figure 4 (Attention エントロピー プロット):**  
  - Plotly により、各レイヤー・ヘッドの Attention エントロピー  
    $$
    H(\alpha_i) = -\sum_{j=1}^{n} \alpha_{ij}\log\alpha_{ij}
    $$
    の分布をプロットします。  
  - さらに、[CLS] トークンに対するエントロピーも別途プロットされ、各層ごとの注意の広がり（集中度）が視覚化されます。

- **Figure 6 (Attention ヘッドのクラスタリング):**  
  - 上記の JS Divergence に基づく距離行列を MDS により 2 次元に埋め込んだ結果を Plotly で表示します。  
  - 各ヘッドは、平均 Attention やエントロピー、特殊トークンへの注意に基づいて色分け・マーカー分けされ、同一層内で類似したヘッドがクラスタとして形成される様子が示されます。

### 意義
- 全体の可視化ページにより、ユーザーは自らテキストを入力して BERT の Attention の性質や各ヘッドの挙動をインタラクティブに確認でき、元論文の理論的知見を体感することができます。

--- -->

## まとめ

本解析の意義は、以下の点に集約されます。

1. **自己教師あり学習による言語構造獲得の解明**  
    - BERT は、明示的な文法情報を与えなくとも膨大なテキストから文法的・意味的な構造を自発的に獲得しています。  
    - Attention マップの各種解析（相対位置、特殊トークンへの注意、エントロピーなど）を通じて、モデル内部でどのような局所的・グローバルな情報が学習されているかが明らかになりました。

2. **Attention マップの解釈可能性の向上**  
    - Attention 重み \( \alpha_{ij} \) は、どのトークンがどの程度他のトークンに依存しているかを示す明瞭な指標であり、これを元に各種統計量（平均 Attention、エントロピー、JS Divergence）を計算することで、BERT の内部の情報処理メカニズムが直感的に理解できるようになります。

3. **情報の統合と冗長性の理解、及び効率化への示唆**  
    - 単一のヘッドでは部分的な情報しか捉えられないが、Attention-Only Probe や Attention-and-Words Probe により複数ヘッドを統合することで、全体として高い依存解析精度が得られます。  
    - また、同一層内で類似した Attention パターンを示すヘッドが多数存在することは、モデル内部に冗長性があることを示し、不要なヘッドのプルーニングやモデル圧縮に向けた示唆を提供します。

4. **実装していない評価表の意義**  
    - 論文では Table 1 や Table 3 のように、各依存関係ごとの精度やプロービング分類器の評価結果が示されています。  
    - これらは、Attention マップに含まれる文法情報の豊かさや、複数ヘッドの統合の効果を定量的に評価するための重要な指標となり、Attention の実用性を裏付ける結果となっています。

5. **総合的な可視化の意義**  
    - Streamlit を用いた全体の可視化ページにより、ユーザーは自ら入力テキストを指定して、Figure 1（Attention マップの例）、Figure 2（平均 Attention プロット）、Figure 4（Attention エントロピー プロット）、Figure 6（Attention ヘッドのクラスタリング）などの結果をインタラクティブに確認できます。  
    - これにより、BERT の内部 Attention がどのように文法・意味情報を捉えているかを、実際に体感しながら理解することができます。

---


# 未実装部分

このセクションでは、各 Attention ヘッドがどの程度、文法的依存関係やコアリファレンスといった言語現象を捉えているかを定量的に評価します。

### 2.1 プロービングの方法

#### 理論的背景
- **バイトペアトークナイゼーションの影響**  
  BERT は byte-pair tokenization を用いるため、単語が複数のサブトークンに分割される場合があります。そのため、各サブトークンごとの注意重み \( \alpha_{ij} \) を「合計」または「平均」することで、単語単位の注意分布を再構築します。  
  これにより、元の単語全体に対する注意の合計が 1 になる性質を保持します。

- **単一ヘッドによる依存関係の予測**  
  各ヘッドについて、入力単語 \( j \) に対して最も高い注意重み \( \alpha_{ij} \) を持つ単語 \( i \) を、その単語 \( j \) の予測ヘッド（依存先）とします：
  \[
  \hat{i} = \operatorname{argmax}_i \alpha_{ij}
  \]
  このシンプルな手法でも、たとえば「det（決定詞）」「dobj（直接目的語）」など、特定の依存関係において 75～95% の高精度が得られることが示されています。

### 2.2 依存構文解析への応用

#### 理論的背景
- **評価方法**  
  Wall Street Journal の Penn Treebank（Stanford Dependencies のアノテーション付き）を用いて、各ヘッドが予測する依存関係とゴールドスタンダードとを比較します。  
  評価には Unlabeled Attachment Score (UAS) を使用します。UAS は以下の式で定義されます：
  \[
  \text{UAS} = \frac{1}{|T|} \sum_{t \in T} \mathbb{I}[P(t) = G(t)]
  \]
  ここで \( P(t) \) は予測されたヘッド、\( G(t) \) はゴールドのヘッド、\(\mathbb{I}\) は指示関数です。ラベルは無視され、単にヘッドが正しく予測されたかどうかのみが評価されます。

- **方向性の違い**  
  実験では、「ヘッドから依存語への注意」と「依存語からヘッドへの注意」の両方向を考慮し、どちらがより高い精度を示すかを検証しています。  
  結果、依存語が自らのヘッドに注意を向ける場合の方が、より高い精度を示す傾向が明らかになりました。

### 2.3 コアリファレンス解析への応用

#### 理論的背景
- **タスク設定**  
  コアリファレンス解析では、ある言及（例えば代名詞）が文中のどの先行言及と最も強い注意を持つかを予測します。  
  この際、ルールベースの手法や「最も近い候補」などのベースラインと比較されます。

- **評価指標**  
  先行言及選択の正解率を測定することで、Attention ヘッドがどの程度コアリファレンス情報を捉えているかを定量的に評価します。  
  特定のヘッドがルールベース手法を上回る性能を示す場合、BERT の Attention マップが意味的・文脈的情報を捉えていることを裏付けます。
